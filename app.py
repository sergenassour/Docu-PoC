from dotenv import load_dotenv

load_dotenv()
import os

import streamlit as st
from langchain_community.chat_models.openai import ChatOpenAI
from langchain_core.messages import HumanMessage

from parsing import read_file

st.set_page_config(layout="wide")

st.header("Chat With Your Documents")
st.subheader(":red[Do not upload private or sensitive information.]")

uploaded_files = st.file_uploader("Upload a PDF or DOCX file", type=["pdf", "docx"], accept_multiple_files=True)

if not uploaded_files or len(uploaded_files) == 0:
    st.stop()

try:
    files = [read_file(uploaded_file) for uploaded_file in uploaded_files]
    all_texts = "\n\n".join([f"=========={file.name}\n{file.get_all_texts()}\n==========" for file in files])
except Exception as e:
    st.error("Error reading file.")
    st.stop()

st.divider()

col1, col2 = st.columns(2)

llm = ChatOpenAI(
    model="gpt-4-0125-preview",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    streaming=True
)


@st.cache_data
def Get_Suggestions():
    return llm.invoke([HumanMessage(
        "Documents: " + all_texts + "\n\nGive me two insightful yet simple questions about the document. Be brief and concise, and return each question with a newline. Do not include any other information or formatting.")])


questions = Get_Suggestions()

st.subheader("Insightful Questions About Your Document Generated by AI")
st.write(questions.content)

st.divider()

st.subheader("Chat")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("What would you like to ask about your document?"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    messages_with_system_prompt = [
        {
            "role": "system",
            "content": "You are a helpful assistant meant to assist users with their documents." +
                       "Here is the document uploaded by the user:\n\n" + all_texts +
                       "\n\nBe brief and concise in your responses."
        },
        *st.session_state.messages]

    response = llm.stream(messages_with_system_prompt)

    with st.chat_message("assistant"):
        response = st.write_stream(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
